## Script

Welcome to my presentation on my investigation into efficiently allocating renewable energy sources across the UK.

So what are the problems we're attempting to solve in this project?

Well, in todays world, due to the threat of climate change there is ever more demand for renewable energy. However the majority of renewable energy sources are reliant on the weather for their output, introducing an inherent uncertainty and inconsistency. We want to tackle this uncertainty and build some method of predicting the output based on the weather.

The other problem we will be approaching is that the search space for this problem grows exponentially as more candidate locations are added to consideration. Combined with the uncertainty of how well a given allocation will perform this is not something that can easily be worked out by brute force and therefore we propose an AI based solution to these problems.

Before beginning our development we needed to research a few areas to inform our decision making. First we considered which renewable energy sources would be applicable to this sort of problem, and found that the main ones to look further into would be Wind & Solar. Each of these types of sources are impacted by different aspects of the weather, intuitively the main factors of each being Wind Speed and sunlight time as their names would suggest. Considering the both Wind & Solar we have to determine if they are both applicable to the scale we aim to achieve, in a cost perspective wind is cheaper per MW and is more commonly deployed on a large scale whereas large scale solar farms need a lot of space and are often more suitable for individuals to have installed on their property. Finally when researching the background it is important to know what standards we need to achieve to consider our project an improvement over the status quo. Within the industry of renewable energy the term 'Load Factor' refers to the percentage of its maximum capacity a generator outputs. Using this as a metric we can see that existing onshore wind turbines had a load factor of 26.5% in 2019. Setting us a target to improve upon.

From our research we determined that a focus on wind turbines would be most suitable for this project as an initial look at the feasibility and performance of our approaches. This is because at the scale we would want to propose solar energy is not all appropriately scalable due to the amount of land required to match the generation capacity of a set of large wind turbines. Another factor in our decision was that the generation of solar cells if very much strictly dependent on the amount of sunlight a quality that does not vary all that much across regions of the UK whereas factors such as wind speed and air pressure are much more variable across altitudes and locations. Furthermore this will also only consider onshore wind, as considering offshore wind would require accurate data that is much less accessible, impacting our ability to build a dataset. The last constraint we introduce is that in our allocation we will work with a predefined set of locations based upon already existing windfarms so we can remove factors such as planning permission and other associated costs & issues from our model.

We'll now briefly cover our overall approach to this project before going into more detail about each step. Firstly we will be training a regression model that will map a set of weather features to the load factor allowing us to predict the output of a location based on the weather. Next we will model the "expected weather" for each of our candidate locations, this will allow us to sample a set of 'expected days' to test the out allocations against. Then finally these previous two steps will be combined into the fitness heuristic that will inform our allocation system. This will aim to maximise a set of objectives and return us a set of results that it has deemed the most performant for review.

Before developing anything we needed to build our datasets, for our training and testing set we need two components, weather features at a point in time and then the actual amount generated by the turbine at that time as a target value. We sourced these values from reliable APIs allowing us to build a large dataset 7000+ entry dataset to test and train upon. We also collected daily weather averages of each candidate location for the past 2 years to allow us to construct our "expected weather" model.

Next we constructed our list of locations, this was initially extracted from our available data to ensure that each location had suitable data available add to the training set. We then went over the list and refined it to remove any locations that were offshore taking our list of locations to 56.

Here you can see a sample of the training set, each entry contains 7 features and one target values as well as an id refering to the specific wind farm. 

Next we will discuss the regression model and our approach to getting the best possible results. We tested a variety of standard regression models and evaluated them based on the mean squared error of our testing set which was sampled from the full dataset. We found that non-linear approaches such a gradient boosting based techniques tended to perform better but to improve further we needed a larger pipeline of techniques. A pipeline is a combination of multiple steps where the output of one step is passed to the input of the next, allowing for the combination of multiple pre and post processing steps to improve prediction. We used an open source library for this which gave us another boost in accuracy.

 Happy with our regressor model, we moved onto the allocation task. here we define our task, we want to give our system a list of objectives to maximise, a budget - meaning number of turbines total we are allowed to place (under the assumption that every turbine has an equal capacity), and the set of candidate locations and the system should return to us a list of values corresponding to the number of turbines to place at each location.

What makes this problem slightly different to a standard search is that we want to optimise for multiple objectives, namely the output of our allocation (budget used x mean load factor), the varience in load factor, and the min & max load factor. Combined, these optimising these objectives should result in an allocation that will have the highest output while still remaining consistent, exactly the qualities we want.

To implement these we elected to take a genetic algorithm approach, using the well establish NGSA-II algorithm. This ranks the population based on how well the perform relative to every other member of the population. A member i dominates member j if for each objective i is at least as good as j AND it it strictly better in at least one objective. This allows us to define "fronts" based on the number of members each member is dominated by. In cases where we need tie breakers we want to encourage exploration in the set so we rank members who are more isolated higher to reduce crowding. Using these metrics we can select our best members of each population and let them reproduce. This will not give us one explicit "correct" answer as our rankings rank based on domination, so we will get a list of solutions which dominate all others and we can examine to determine which gives us the focus we desire.

This approach is not the perfect solution however, NGSA-II is by no means a cutting edge algorithm any more and it along with evolutionary algorithms in general tend to suffer in the time department. Due to the large search space the solutions found in a finite number of iterations is restricted based on the starting seed, meaning to find the best solutions we will need to run a large number of batches to be confident in our "best" outputs. We believe this trade off is acceptable though as this task is not time critical and will likely need to be ran relatively irregularly. We also believe there is much room for performance improvements in future work by parallelising batches and investigating modifications to NGSA-II.

Every initial seed explores differently, we can visualise how it changes generation to generation by plotting the average objective value within the top-cut and seeing how it generally trends up with some occasional dips. Going back to the "fronts" way of visualising the selection process it is understandable that some population members may sacrifice one objective value to dominate in another dragging the mean value of that objective down. this just means that to see which results best balance the objective values in a appealing way requires a bit more of a human touch.

Once we have examined our set of dominant allocations and picked it we can visualise the locations to examine the trends in the algorithms choices. We see that it picks a very sparse selection of locations usually around 10-15 of the 56 possible, despite it having plenty of budget to pick more. We also see that it tends to prefer more coastal allocations despite there being a few in land choices. The final observation we have made at this stage is that most allocations consist of few more heavily weighted choices to increase the mean and less weighted ones which act as an almost "stabilising factor" to reduce the variance that would come from relying on one location.





